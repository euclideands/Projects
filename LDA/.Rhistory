# load data
x<-read.table("https://github.com/QuantLet/MVA/blob/master/QID-1202-MVAclusbank/bank2.dat")
xg      = x[1:100, ]
xf      = x[101:200, ]
mg      = apply(xg, 2, mean)    # mean forged
mf      = apply(xf, 2, mean)    # mean genuine
m       = (mf + mg)/2           # total mean
s       = (99 * cov(xg) + 99 * cov(xf))/198  # sd
x<-read.table("https://github.com/QuantLet/MVA/blob/master/QID-1202-MVAclusbank/bank2.dat")
x
x <- read.table("https://github.com/QuantLet/MVA/blob/master/QID-1202-MVAclusbank/bank2.dat")
x
url <- "https://github.com/QuantLet/MVA/blob/master/QID-1202-MVAclusbank/bank2.dat"
x <- read.table(url)
x
url <- "https://github.com/QuantLet/MVA/blob/master/QID-2454-MVAaper/bank2.dat"
x <- read.table(url)
x
# clear all variables
rm(list = ls(all = TRUE))
graphics.off()
# load data
x       = read.table("https://github.com/QuantLet/MVA/blob/master/QID-2454-MVAaper/bank2.dat")
xg      = x[1:100, ]
xf      = x[101:200, ]
mg      = apply(xg, 2, mean)    # mean forged
mf      = apply(xf, 2, mean)    # mean genuine
m       = (mf + mg)/2           # total mean
s       = (99 * cov(xg) + 99 * cov(xf))/198  # sd
# clear all variables
rm(list = ls(all = TRUE))
graphics.off()
# load data
data<-read.table("C://Users//BS503TX//Downloads//bostonh.dat")
# Clear all variables
rm(list = ls(all = TRUE))
graphics.off()
library("readr")
library("tools")
library("HSAUR2")
library("MVA")
# Data visualization
library("ggplot2")
# LDA model
library("MASS")
# Clear all variables
rm(list = ls(all = TRUE))
graphics.off()
library("readr")
library("tools")
library("HSAUR2")
library("MVA")
# Data visualization
library("ggplot2")
# LDA model
library("MASS")
url <- "https://raw.githubusercontent.com/QuantLet/MVA/master/QID-2298-MVAboxbhd/bostonh.dat"
# Read the dataset from the URL and specify column names
df <- read.table(url)
# Rename the columns
col_names <- c()
for (i in 1:ncol(df)) {
col_names <- c(col_names, paste("x", i, sep = ""))
}
colnames(df) <- col_names
# View the first few rows of the dataset
head(df)
# Check for null values in a dataframe
null_values <- is.na(df)
# Sum the null values in each column
col_null_counts <- colSums(null_values)
# Display the number of null values in each column
print(col_null_counts)
# # Remove rows with null values from the dataframe
# df <- na.omit(df)
# Check for duplicates in the entire dataframe
duplicates <- duplicated(df)
# Display the rows with duplicate values
print(df[duplicates, ])
# # Remove duplicates and keep the first occurrence
# df <- unique(df)
# Path to output pdf
# pdf("D:/bostonhousing_distplot.pdf")
# Set the number of rows and columns for the grid
num_rows <- 3
num_cols <- 5
# Set the plot size
options(repr.plot.width = 10, repr.plot.height = 10)
# Create a 4x3 grid of subplots
par(mfrow = c(num_rows, num_cols))
# Get all numeric columns from the data frame df
numeric_columns <- df[, sapply(df, is.numeric)]
# Loop through the numeric columns and create density plots with histograms
for (i in 1:ncol(numeric_columns)) {
col_name <- names(numeric_columns)[i]
col_data <- numeric_columns[, i]
# Create the histogram
hist(col_data, col = "lightblue", border = "lightblue", prob = TRUE,
xlab = "Value", main = paste("Distplot for", col_name))
# Create the density plot
density_values <- density(col_data)
lines(density_values, col = "blue")
}
transformed_df <- df
# Perform transformations
transformed_df[, 1] <- log(df[, 1])
transformed_df[, 2] <- df[, 2] / 10
transformed_df[, 3] <- log(df[, 3])
transformed_df[, 5] <- log(df[, 5])
transformed_df[, 6] <- log(df[, 6])
transformed_df[, 7] <- (df[, 7] ^ (2.5)) / 10000
transformed_df[, 8] <- log(df[, 8])
transformed_df[, 9] <- log(df[, 9])
transformed_df[, 10] <- log(df[, 10])
transformed_df[, 11] <- exp(0.4 * df[, 11]) / 1000
transformed_df[, 12] <- df[, 12] / 100
transformed_df[, 13] <- sqrt(df[, 13])
transformed_df[, 14] <- log(as.numeric(df[, 14]))
# Set the number of rows and columns for the grid
num_rows <- 3
num_cols <- 5
# Set the plot size
options(repr.plot.width = 10, repr.plot.height = 10)
# Create a 4x3 grid of subplots
par(mfrow = c(num_rows, num_cols))
# Get all numeric columns from the data frame df
numeric_columns <- df[, sapply(df, is.numeric)]
# Loop through the numeric columns and create density plots with histograms
for (i in 1:ncol(numeric_columns)) {
col_name <- names(numeric_columns)[i]
col_data <- numeric_columns[, i]
# Create the histogram
hist(col_data, col = "lightblue", border = "lightblue", prob = TRUE,
xlab = "Value", main = paste("Distplot for", col_name))
# Create the density plot
density_values <- density(col_data)
lines(density_values, col = "blue")
}
# Set the number of rows and columns for the grid
num_rows <- 3
num_cols <- 5
# Set the plot size
options(repr.plot.width = 10, repr.plot.height = 10)
# Create a 4x3 grid of subplots
par(mfrow = c(num_rows, num_cols))
# Get all numeric columns from the data frame df
numeric_columns <- transformed_df[, sapply(transformed_df, is.numeric)]
# Loop through the numeric columns and create density plots with histograms
for (i in 1:ncol(numeric_columns)) {
col_name <- names(numeric_columns)[i]
col_data <- numeric_columns[, i]
# Create the histogram
hist(col_data, col = "lightblue", border = "lightblue", prob = TRUE,
xlab = "Value", main = paste("Distplot for", col_name))
# Create the density plot
density_values <- density(col_data)
lines(density_values, col = "blue")
}
# Drop x4 from df
df = transformed_df
df <- df[, -4]
head(df)
# Boxplot
options(repr.plot.width = 10, repr.plot.height = 10)
boxplot(transformed_df, at = 1:14, axes = FALSE, main = "Boston Housing data", cex.main = 1.5)
for (i in 1:14) {
lines(c(i - 0.4, i + 0.4), c(mean(transformed_df[, i]), mean(transformed_df[, i])), col = "red3", lty = "dotted",
lwd = 1.2)
}
# Standardization using z-score
standardized_df <- scale(transformed_df)
df <- scale(df)
# Convert to data frame
data <- as.data.frame(standardized_df)
df <- as.data.frame(df)
# transformed_df -> ada x4, sudah ditransformasi
# df -> tidak ada x4, sudah ditansformasi & scaling
# standardized_df -> ada x4, sudah ditransformasi & scaling
# Before-after scaling visualization
par(mfrow = c(2, 1), ask = FALSE, cex = 0.5)
boxplot(transformed_df, at = 1:14, axes = FALSE, main = "Boston Housing data before scaling", cex.main = 1.5)
for (i in 1:14) {
lines(c(i - 0.4, i + 0.4), c(mean(transformed_df[, i]), mean(transformed_df[, i])), col = "red3", lty = "dotted",
lwd = 1.2)
}
boxplot(data, at = 1:14, axes = FALSE, main = "Boston Housing data after scaling", cex.main = 1.5)
for (i in 1:14) {
lines(c(i - 0.4, i + 0.4), c(mean(data[, i]), mean(data[, i])), col = "red3", lty = "dotted",
lwd = 1.2)
}
d    = dist(df, "euclidean", p = 2)   # euclidean distance matrix
w    = hclust(d, method = "ward.D")   # cluster analysis with ward algorithm
tree = cutree(w, 2)
t1   = subset(df, tree == 1)
t2   = subset(df, tree == 2)
# Dendrogram for df after Ward algorithm
plot(w, hang = -0.1, labels = FALSE, frame.plot = TRUE, ann = FALSE, cex.axis = 1.2)
title(main = "Ward method", ylab = "distance", cex.lab = 1.2, cex.main = 1.6)
# Count observations in each cluster
cluster_counts <- table(tree)
count_cluster_1 <- cluster_counts[1]
count_cluster_2 <- cluster_counts[2]
print(count_cluster_1)
print(count_cluster_2)
# Means for Cluster 1 and Cluster 2
mc = cbind(colMeans(subset(df, tree == "1")), colMeans(subset(df, tree == "2")))
mc
# Standard deviations for Cluster 1 and Cluster 2
sd_t1 <- sapply(t1[, 1:ncol(df)], sd)
sd_t2 <- sapply(t2[, 1:ncol(df)], sd)
# Combine standard deviations into a matrix
sc <- cbind(sd_t1, sd_t2)
sc
# Means and standard deviations of the 13 standardized variables for Cluster 1
# (251 observations) and Cluster 2 (255 observations)
tbl = cbind(mc[, 1], sc[, 1]/sqrt(nrow(t1)), mc[, 2], sc[, 2]/sqrt(nrow(t2)))
tbl
# Convert to numeric matrix
df <- as.matrix(df)
# spectral decomposition
eig = eigen(cov(df))
eva = eig$values
eve = eig$vectors[, 1:2]
dav = df %*% eve
# Update the 'tr' variable, not 'tree'
tr = tree
tr[tr == 1] = "red"
tr[tr == 2] = "black"
# Scatterplot for the first two PCs displaying the two clusters
plot(dav[, 1], dav[, 2], pch = tree, col = tr, xlab = "PC1", ylab = "PC2", main = "first vs. second PC",
cex.main = 1.8, cex.axis = 1.4, cex.lab = 1.4)
dat = standardized_df
t3 = subset(dat, tree == 1)
t4 = subset(dat, tree == 2)
t = tree
t[t == 1] = "red"
t[t == 2] = "black"
dat = standardized_df
t3 = subset(dat, tree == 1)
t4 = subset(dat, tree == 2)
t = tree
t[t == 1] = "red"
t[t == 2] = "black"
par(mfrow = c(2, 7), cex = 0.3)
boxplot(t3[, 1], t4[, 1], border = c("red", "black"), xlab = "X1", cex.lab = 3, cex.axis = 3)
boxplot(t3[, 2], t4[, 2], border = c("red", "black"), xlab = "X2", cex.lab = 3, cex.axis = 3)
boxplot(t3[, 3], t4[, 3], border = c("red", "black"), xlab = "X3", cex.lab = 3, cex.axis = 3)
boxplot(t3[, 4], t4[, 4], border = c("red", "black"), xlab = "X4", cex.lab = 3, cex.axis = 3)
boxplot(t3[, 5], t4[, 5], border = c("red", "black"), xlab = "X5", cex.lab = 3, cex.axis = 3)
boxplot(t3[, 6], t4[, 6], border = c("red", "black"), xlab = "X6", cex.lab = 3, cex.axis = 3)
boxplot(t3[, 7], t4[, 7], border = c("red", "black"), xlab = "X7", cex.lab = 3, cex.axis = 3)
boxplot(t3[, 8], t4[, 8], border = c("red", "black"), xlab = "X8", cex.lab = 3, cex.axis = 3)
boxplot(t3[, 9], t4[, 9], border = c("red", "black"), xlab = "X9", cex.lab = 3, cex.axis = 3)
boxplot(t3[, 10], t4[, 10], border = c("red", "black"), xlab = "X10", cex.lab = 3,
cex.axis = 3)
boxplot(t3[, 11], t4[, 11], border = c("red", "black"), xlab = "X11", cex.lab = 3,
cex.axis = 3)
boxplot(t3[, 12], t4[, 12], border = c("red", "black"), xlab = "X12", cex.lab = 3,
cex.axis = 3)
boxplot(t3[, 13], t4[, 13], border = c("red", "black"), xlab = "X13", cex.lab = 3,
cex.axis = 3)
boxplot(t3[, 14], t4[, 14], border = c("red", "black"), xlab = "X14", cex.lab = 3,
cex.axis = 3)
# Scatterplot matrix for variables X1 to X7
pairs(transformed_df[, 1:7], col = tr, upper.panel = NULL, labels = c("X1", "X2", "X3", "X4",
"X5", "X6", "X7"), cex.axis = 1.2)
# Scatterplot matrix for variables X8 to X14
pairs(transformed_df[, 8:14], col = tr, upper.panel = NULL, labels = c("X8", "X9", "X10", "X11",
"X12", "X13", "X14"), cex.axis = 1.2)
m1  = colMeans(t1) # mean of first cluster
m2  = colMeans(t2) # mean of second cluster
m   = (m1 + m2)/2  # mean of both clusters
# common variance matrix
s = ((nrow(t1) - 1) * cov(t1) + (nrow(t2) - 1) * cov(t2)) / (nrow(transformed_df) - 2)
# alpha for the discrimination rule
alpha = solve(s) %*% (m1 - m2)
t1 <- as.matrix(t1)
t2 <- as.matrix(t2)
# APER for clusters of Boston houses
mis1  = sum((t1 - m) %*% alpha < 0)     # misclassified 1
mis2  = sum((t2 - m) %*% alpha > 0)     # misclassified 2
corr1 = sum((t1 - m) %*% alpha > 0)     # correct 1
corr2 = sum((t2 - m) %*% alpha < 0)     # correct 2
# APER (apparent error rate)
aper  = (mis1 + mis2)/nrow(transformed_df)
alph  = (df - matrix(m, nrow(df), ncol(df), byrow = T)) %*% alpha
set.seed(1)
# discrimination scores
p = cbind(alph, tree + 0.05 * rnorm(NROW(tree)))
tree2 = tree
tree2[tree2 == 1] = "red"
tree2[tree2 == 2] = "black"
# plot of discrimination scores
plot(p[, 1], p[, 2], pch = tree, col = tree2, xaxt = "n", yaxt = "n", xlab = "", ylab = "",
bty = "n")
abline(v = 0, lwd = 3)
title(paste("Discrimination scores"))
print(aper)
data = transformed_df
n = nrow(data)
# AER for clusters of Boston houses
i     = 0
mis1  = 0
mis2  = 0
corr1 = 0
corr2 = 0
while (i < n) {
i     = i + 1
xi    = subset(df, 1:n != i)
treei = subset(tree, 1:n != i)
t1    = subset(xi, treei == 1)
t2    = subset(xi, treei == 2)
m1    = colMeans(t1)                # mean of first cluster
m2    = colMeans(t2)                # mean of second cluster
m     = (m1 + m2)/2                 # mean of both clusters
s     = ((nrow(t1) - 1) * cov(t1) + (nrow(t2) - 1) * cov(t2))/(nrow(df) - 2)    # common variance matrix
alpha = solve(s) %*% (m1 - m2)                                                  # alpha for the discrimination rule
mis1  = mis1 + (tree[i] == 1) * ((df[i, ] - m) %*% alpha < 0)                   # misclassified 1
mis2  = mis2 + (tree[i] == 2) * ((df[i, ] - m) %*% alpha > 0)                   # misclassified 2
corr1 = corr1 + (tree[i] == 1) * ((df[i, ] - m) %*% alpha > 0)                  # correct 1
corr2 = corr2 + (tree[i] == 2) * ((df[i, ] - m) %*% alpha < 0)                  # correct 2
}
aer = (mis1 + mis2)/nrow(df)            # AER (actual error rate)
print(aer)
